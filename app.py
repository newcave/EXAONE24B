import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

# âœ… ëª¨ë¸ ì €ì¥ ê²½ë¡œ ë° Hugging Face ëª¨ë¸ëª…
MODEL_NAME = "LGAI-EXAONE/EXAONE-Deep-2.4B"

@st.cache_resource  # ëª¨ë¸ ìºì‹±í•˜ì—¬ ë¡œë“œ ì†ë„ í–¥ìƒ
def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œ"""
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    model.eval()
    return tokenizer, model

# ëª¨ë¸ ë¡œë“œ
tokenizer, model = load_model()

# âœ… ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(prompt):
    """EXAONE ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±"""
    device = next(model.parameters()).device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=1500,
            do_sample=False,
            temperature=0.1,
            top_p=0.9,
            repetition_penalty=1.2
        )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# âœ… Streamlit UI êµ¬ì„±
st.title("ğŸ§  EXAONE-Deep-2.4B Chatbot")
st.write("ğŸ’¬ **Fact ê¸°ë°˜ AI ì±—ë´‡**ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# ì±„íŒ… ì…ë ¥ì°½
user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", "")

# ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ ì‘ë‹µ ìƒì„±
if user_input:
    with st.spinner("EXAONEì´ ë‹µë³€ì„ ìƒì„± ì¤‘..."):
        response = generate_response(user_input)
    st.markdown(f"### ğŸ¤– EXAONEì˜ ë‹µë³€:")
    st.write(response)
