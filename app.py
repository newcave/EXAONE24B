import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer

# âœ… ëª¨ë¸ëª… ë° ë¡œì»¬ ìºì‹œ ì„¤ì •
MODEL_NAME = "LGAI-EXAONE/EXAONE-Deep-2.4B"

@st.cache_resource  # âœ… Streamlit ìºì‹± (ìµœì´ˆ ì‹¤í–‰ í›„ ìœ ì§€)
def load_model():
    """EXAONE ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (ìµœì´ˆ ì‹¤í–‰ ì‹œë§Œ ë¡œë“œ)"""
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME, trust_remote_code=True, revision="main"
    )
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        torch_dtype=torch.float16,  # GPU ìµœì í™”
        device_map="auto",  # GPU ìë™ í• ë‹¹
        trust_remote_code=True,
        revision="main"
    )
    model.eval()
    return tokenizer, model

# âœ… ëª¨ë¸ ë¡œë“œ (Streamlit ìºì‹± ì ìš©)
tokenizer, model = load_model()

# âœ… ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(prompt):
    """EXAONE ëª¨ë¸ì´ ë‹µë³€ì„ ìƒì„±"""
    device = next(model.parameters()).device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=1000,  # ë„ˆë¬´ ê¸¸ë©´ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ ê°€ëŠ¥
            do_sample=False,  # ëœë¤ì„± ì œê±°
            temperature=0.1,  # ì°½ì˜ì„± ìµœì†Œí™”
            top_p=0.9,  # í•µì‹¬ ì •ë³´ ìœ„ì£¼
            repetition_penalty=1.2  # ë°˜ë³µ ë°©ì§€
        )
    return tokenizer.decode(output[0], skip_special_tokens=True)

# âœ… Streamlit UI êµ¬ì„±
st.title("ğŸ§  EXAONE-Deep-2.4B Chatbot")
st.write("ğŸ’¬ **Fact ê¸°ë°˜ AI ì±—ë´‡**ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.")

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", "")

# ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ ì‘ë‹µ ìƒì„±
if user_input:
    with st.spinner("EXAONEì´ ë‹µë³€ì„ ìƒì„± ì¤‘..."):
        response = generate_response(user_input)
    st.markdown("### ğŸ¤– EXAONEì˜ ë‹µë³€:")
    st.write(response)
